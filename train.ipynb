{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of train_cmaterdb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjcXuUD905_q"
      },
      "source": [
        "---\n",
        "> (Call load_dataset before any operation because of RAM overflow issue) \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shw3om3Vt4vZ"
      },
      "source": [
        "---\n",
        "> ***Import Modules***\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdKIiCNesGA8"
      },
      "source": [
        "# .cuda() for using cuda enabled NVIDIA GPU to compute\n",
        "# erase .cuda() if you haven't cuda enabled NVIDIA GPU\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os.path\n",
        "from Model import Model\n",
        "import pandas as pd \n",
        "from torch.utils.data import Dataset\n",
        "import csv\n",
        "import csv_loader\n",
        "\n",
        "to_bangla = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYzYKRxjtZh5"
      },
      "source": [
        "---\n",
        "> ***Function for load and save checkpoints***\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEn04rCDtC6e"
      },
      "source": [
        "# saving training checkpoints\n",
        "def save_checkpoint(epoch, model_state_dict, criterion_state_dict, optim_state_dict, \n",
        "                    trainloss_list, trainac_list, valloss_list, valac_list, best_valloss, PATH):\n",
        "    torch.save({\n",
        "        'epoch':epoch,\n",
        "        'model_state_dict':model_state_dict,\n",
        "        'criterion_state_dict':criterion_state_dict,\n",
        "        'optimizer_state_dict':optim_state_dict,\n",
        "        'trainloss_list':trainloss_list,\n",
        "        'trainac_list':trainac_list,\n",
        "        'valloss_list':valloss_list,\n",
        "        'valac_list':valac_list,\n",
        "        'best_valloss':best_valloss\n",
        "    }, PATH)\n",
        "\n",
        "\n",
        "# loading training checkpoints\n",
        "def load_checkpoint(model, criterion, optimizer, PATH):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
        "\n",
        "    return checkpoint['epoch'], model, criterion, optimizer, checkpoint['trainloss_list'], checkpoint['trainac_list'], checkpoint['valloss_list'], checkpoint['valac_list'], checkpoint['best_valloss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfFcMy6stWGS"
      },
      "source": [
        "---\n",
        "> Function to Load Dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRWzMmejtI1t"
      },
      "source": [
        "# loading datasets\n",
        "def load_dataset():\n",
        "    global trainset, valset, train_dataloader, val_dataloader, test_dataloader\n",
        " \n",
        "    trainset_path = 'train_set.csv'\n",
        "    testset_path = 'test_set.csv'\n",
        "    # read from csv\n",
        "    trainset, testset = csv_loader.read_from_csv(trainset_path, testset_path)\n",
        "\n",
        "    # spilitting trainset by assigning 10% to valset and 90% to trainset\n",
        "    valset_size = int(0.1 * len(trainset)); rest_size = len(trainset) - valset_size\n",
        "    valset, trainset = torch.utils.data.random_split(trainset, [valset_size, rest_size])\n",
        "\n",
        "    aug_trainset1 = csv_loader.read_from_csv_aug_lu(trainset_path)\n",
        "    aug_trainset2 = csv_loader.read_from_csv_aug_ru(trainset_path)\n",
        "    aug_trainset3 = csv_loader.read_from_csv_aug_lb(trainset_path)\n",
        "    aug_trainset4 = csv_loader.read_from_csv_aug_rb(trainset_path)\n",
        "\n",
        "    aug_trainset = torch.utils.data.ConcatDataset([aug_trainset1, aug_trainset2,aug_trainset3, aug_trainset4])\n",
        "\n",
        "    #choosing random 60000 images\n",
        "    #aug_trainset, bakp = torch.utils.data.random_split(aug_trainset, [60000, len(aug_trainset)-60000])\n",
        "\n",
        "    aug_size = int(0.1 * len(aug_trainset)); rest_size = len(aug_trainset) - aug_size\n",
        "    aug_valset, aug_trainset = torch.utils.data.random_split(aug_trainset, [aug_size, rest_size])\n",
        "\n",
        "    trainset = torch.utils.data.ConcatDataset([trainset, aug_trainset])\n",
        "    valset = torch.utils.data.ConcatDataset([valset, aug_valset])\n",
        "\n",
        "    # construct dataloaders\n",
        "    train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "    val_label_set = set()\n",
        "    for i in range(0, len(valset)):\n",
        "        val_label_set.add(str(valset[i][1]))\n",
        "    print(\"Distinct character in valset:\", len(val_label_set))\n",
        "    print(len(trainset), len(valset))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nUrPkU6tZhJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UawZ6mDqydkg"
      },
      "source": [
        "> Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P279PkSSwLOu"
      },
      "source": [
        "model = Model().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwVFW50cyZKo"
      },
      "source": [
        "> Call Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Tm_SanwN8P"
      },
      "source": [
        "load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "172gjo9g0Csl"
      },
      "source": [
        "> Function to plot Train and validation loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNE-CfhJyS55"
      },
      "source": [
        "# graphical representation of train loss & validation loss\n",
        "def plot_train_and_validation_data(epoch, train_data, valid_data, type=\"Loss\"):\n",
        "\n",
        "    # plotting Train Loss & Validation Loss data\n",
        "    fig = plt.figure(figsize=(13,5))\n",
        "    plt.plot(np.arange(0, epoch), train_data, label=\"Train \" + type, linewidth=3)\n",
        "    plt.plot(np.arange(0, epoch), valid_data, label=\"Validation \" + type, linewidth=3)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(type)\n",
        "    plt.title(type + \" Plots\")\n",
        "    if type == \"Loss\":\n",
        "        plt.legend(loc='upper right')\n",
        "    else:\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "    leg_texts = plt.legend().get_texts()\n",
        "    plt.setp(leg_texts, fontsize='x-large')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx8nzt8O0MSV"
      },
      "source": [
        "> Train and validation function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78mAGJYh0J5x"
      },
      "source": [
        "# this function will perform training & validation\n",
        "def train_and_validation(model):\n",
        "    global trainset, valset, train_dataloader, val_dataloader\n",
        "\n",
        "    # defining loss criterion function\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # defining optimizer with learning rate=0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    trainLoss = list(); trainAccuracy = list()\n",
        "    valLoss = list(); valAccuracy = list()\n",
        "    bestValLoss = 100\n",
        "    epoch = 0\n",
        "    totalEpoch =  100   # higher value will perform more training but take more time\n",
        "    checkpoint_path = './data/checkpoint_bd_char_testing.pt'\n",
        "\n",
        "    # Restoring training checkpoint if available\n",
        "    if(os.path.isfile(checkpoint_path)):\n",
        "        epoch, model, criterion, optimizer, trainLoss, trainAccuracy, valLoss, valAccuracy, bestValLoss = load_checkpoint(model, criterion, optimizer, checkpoint_path)\n",
        "\n",
        "    while epoch <= totalEpoch:\n",
        "        totalTrainLoss = 0\n",
        "        totalValLoss = 0; totalMatched = 0\n",
        "\n",
        "        # training mode select\n",
        "        model.train()\n",
        "        model.cuda()\n",
        "\n",
        "        # training starts\n",
        "        for iter, (img, label) in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # pass image to model to get prediction\n",
        "            prediction = model(img)\n",
        "\n",
        "            # calculate loss\n",
        "            loss = criterion(prediction, label).cuda()\n",
        "            totalTrainLoss += loss.item()\n",
        "\n",
        "            prediction = torch.nn.functional.softmax(prediction, dim=1)\n",
        "            for i, pred in enumerate(prediction):\n",
        "                if label[i] == torch.max(pred.data, 0)[1]:\n",
        "                    totalMatched += 1\n",
        "\n",
        "            # do a backward pass using loss value and use Adam optimizer to modify the model parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        taccuracy = totalMatched / len(trainset)\n",
        "        # avergae train loss\n",
        "        totalTrainLoss = totalTrainLoss / (iter + 1)\n",
        "        trainLoss.append(totalTrainLoss); trainAccuracy.append(taccuracy)\n",
        "\n",
        "        # validation mode select\n",
        "        model.eval()\n",
        "        model.cuda()\n",
        "        totalMatched = 0\n",
        "\n",
        "        # validation starts\n",
        "        for iter, (image, label) in enumerate(val_dataloader):\n",
        "            # pass image to model to get prediction\n",
        "            prediction = model(image)\n",
        "\n",
        "            loss = criterion(prediction, label).cuda()\n",
        "            totalValLoss += loss.item()\n",
        "\n",
        "            prediction = torch.nn.functional.softmax(prediction, dim=1)\n",
        "            for i, pred in enumerate(prediction):\n",
        "                if label[i] == torch.max(pred.data, 0)[1]:\n",
        "                    totalMatched += 1\n",
        "        \n",
        "        accuracy = totalMatched / len(valset)\n",
        "        totalValLoss = totalValLoss / (iter+1)\n",
        "        valLoss.append(totalValLoss); valAccuracy.append(accuracy)\n",
        "\n",
        "        print(\"Completed Loop No = \", epoch)\n",
        "        print(\"Train Loss:\", totalTrainLoss, \", Validation Loss:\", totalValLoss, \", Train Accuracy: \", taccuracy, \", Val Accuracy: \", accuracy)\n",
        "        epoch += 1\n",
        "\n",
        "        if totalValLoss < bestValLoss:\n",
        "            bestValLoss = totalValLoss\n",
        "            print(\"Saving Model State with Validation Loss: \", totalValLoss)\n",
        "            torch.save(model.state_dict(), \"./data/model_bd_char_testing.dth\")\n",
        "\n",
        "        # storing checkpoints\n",
        "        save_checkpoint(epoch, model.state_dict(), criterion.state_dict(), optimizer.state_dict(),\n",
        "                        trainLoss, trainAccuracy, valLoss, valAccuracy, bestValLoss, checkpoint_path)\n",
        "\n",
        "    print(\"\\nTraining & Validation Completed\\n\")\n",
        "\n",
        "    plot_train_and_validation_data(epoch, trainLoss, valLoss)\n",
        "    plot_train_and_validation_data(epoch, trainAccuracy, valAccuracy, \"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3kYm8H728c"
      },
      "source": [
        "> Training Starts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8P-51Zg0Z0K"
      },
      "source": [
        "train_and_validation(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E3F2RmAJr8W"
      },
      "source": [
        ">Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxA4IYMN9BWz"
      },
      "source": [
        "# this function will perform the testing of the trained model\n",
        "def test(model):\n",
        "    global test_dataloader\n",
        "\n",
        "    # loading model & selecting mode\n",
        "    model.load_state_dict(torch.load(\"./data/model_bd_char_testing.dth\"))\n",
        "    model.eval().cuda()\n",
        "\n",
        "    result = list(); rows = list()\n",
        "    tot = 0; correct = 0\n",
        "\n",
        "    for iter, (img, label) in enumerate(test_dataloader):\n",
        "        predict = model(img.cuda())\n",
        "        predict = torch.nn.functional.softmax(predict, dim=1)\n",
        "\n",
        "        for i, p in enumerate(predict):\n",
        "            tot += 1\n",
        "            result.append((img[i], torch.max(p.data, 0)[1]))\n",
        "\n",
        "            Label = torch.max(p.data, 0)[1]\n",
        "            #rows.append([tot, to_bangla[ str(Label.item()) ] ])\n",
        "\n",
        "            if label[i] == Label:\n",
        "                correct += 1\n",
        "\n",
        "    test_accuracy = correct / tot\n",
        "    print(\"Test Accuracy: \", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3NRyQH4Jx5s"
      },
      "source": [
        ">Call Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WpwAw4WJzoO"
      },
      "source": [
        "    test(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}